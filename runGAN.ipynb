{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputSize = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import SGD,Adam,RMSprop\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, merge\n",
    "from keras.layers import Conv1D,Conv2D,Conv2DTranspose,Reshape\n",
    "from scipy import io\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from keras import backend as K  \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trainable(model, trainable):\n",
    "    model.trainable = trainable\n",
    "    for l in model.layers:\n",
    "        l.trainable = trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def D_loss(y_true,y_pred):\n",
    "    return 0.5*K.mean((y_pred-y_true)**2,axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_loss(fake_output,true_input):\n",
    "    def lossfun(y_true, y_pred):\n",
    "        return 1*K.mean(K.abs(fake_output-true_input)) \n",
    "    return lossfun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GAN_loss(fake_output,true_input):\n",
    "    def lossfun(y_true, y_pred):\n",
    "        return 0.5*K.mean((((y_pred-y_true)**2)),axis = -1)+100*K.mean(K.abs(fake_output-true_input))\n",
    "    return lossfun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim =RMSprop(lr=0.0002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input is noise and input1 is clean\n",
    "inputs=Input(shape =(inputSize,1,1))\n",
    "inputs1=Input(shape =(inputSize,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputSize = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder is starting here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32, 1, 64)\n",
      "(?, 8, 1, 128)\n",
      "(?, 2, 1, 256)\n"
     ]
    }
   ],
   "source": [
    "#64 kernels of size 31 and strides of 4\n",
    "cov1=(Conv2D(64, 31, strides = 4,padding='same'))(inputs)\n",
    "cov1=(PReLU())(cov1)  \n",
    "print(cov1.shape)\n",
    "#128 kernels of size 31 and strides of 4\n",
    "cov2=(Conv2D(128, 31, strides=4,padding='same'))(cov1)\n",
    "cov2=(PReLU())(cov2)\n",
    "print(cov2.shape)\n",
    "#256 kernels of size 31 and strides of 4\n",
    "cov3=(Conv2D(256, 31, strides=4,padding='same'))(cov2)\n",
    "cov3=(PReLU())(cov3)\n",
    "print(cov3.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder is starting here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python2.7/site-packages/ipykernel_launcher.py:3: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/conda/lib/python2.7/site-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/opt/conda/lib/python2.7/site-packages/ipykernel_launcher.py:6: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  \n",
      "/opt/conda/lib/python2.7/site-packages/ipykernel_launcher.py:9: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "cov4=(Conv2DTranspose(256,31, strides=(1,1),padding='same'))(cov3)\n",
    "cov4=(PReLU())(cov4)\n",
    "z1 = merge([cov3,cov4], mode='sum')\n",
    "cov5=(Conv2DTranspose(128,31, strides=(4,1),padding='same'))(z1)\n",
    "cov5=(PReLU())(cov5)\n",
    "z2=merge([cov2,cov5], mode='sum')\n",
    "cov6=(Conv2DTranspose(64,31, strides=(4,1),padding='same'))(z2)\n",
    "cov6=(PReLU())(cov6)\n",
    "z3=merge([cov1,cov6], mode='sum')\n",
    "cov7=(Conv2DTranspose(16,31, strides=(4,1),padding='same'))(z3)\n",
    "cov7=(PReLU())(cov7)\n",
    "cov8=(Conv2DTranspose(1,31, strides=(1,1),activation='tanh',padding='same'))(cov7)\n",
    "#cov8=(PReLU())(cov8)\n",
    "cov8=(Reshape((inputSize,1)))(cov8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python2.7/site-packages/ipykernel_launcher.py:1: UserWarning: Update your `Model` call to the Keras 2 API: `Model([<tf.Tenso..., outputs=Tensor(\"re...)`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "G = Model([inputs,inputs1],output = cov8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.compile(loss=G_loss(cov8,inputs1),optimizer=optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 128, 1, 1)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                (None, 32, 1, 64)     61568       input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "p_re_lu_1 (PReLU)                (None, 32, 1, 64)     2048        conv2d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                (None, 8, 1, 128)     7872640     p_re_lu_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "p_re_lu_2 (PReLU)                (None, 8, 1, 128)     1024        conv2d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                (None, 2, 1, 256)     31490304    p_re_lu_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "p_re_lu_3 (PReLU)                (None, 2, 1, 256)     512         conv2d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTransp (None, 2, 1, 256)     62980352    p_re_lu_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "p_re_lu_4 (PReLU)                (None, 2, 1, 256)     512         conv2d_transpose_1[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "merge_1 (Merge)                  (None, 2, 1, 256)     0           p_re_lu_3[0][0]                  \n",
      "                                                                   p_re_lu_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTransp (None, 8, 1, 128)     31490176    merge_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "p_re_lu_5 (PReLU)                (None, 8, 1, 128)     1024        conv2d_transpose_2[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "merge_2 (Merge)                  (None, 8, 1, 128)     0           p_re_lu_2[0][0]                  \n",
      "                                                                   p_re_lu_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTransp (None, 32, 1, 64)     7872576     merge_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "p_re_lu_6 (PReLU)                (None, 32, 1, 64)     2048        conv2d_transpose_3[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "merge_3 (Merge)                  (None, 32, 1, 64)     0           p_re_lu_1[0][0]                  \n",
      "                                                                   p_re_lu_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTransp (None, 128, 1, 16)    984080      merge_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "p_re_lu_7 (PReLU)                (None, 128, 1, 16)    2048        conv2d_transpose_4[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTransp (None, 128, 1, 1)     15377       p_re_lu_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)              (None, 128, 1)        0           conv2d_transpose_5[0][0]         \n",
      "====================================================================================================\n",
      "Total params: 142,776,289\n",
      "Trainable params: 142,776,289\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "G.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 128, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 32, 64)            2048      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 64)            256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 32, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 8, 128)            254080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8, 128)            512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 8, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 2, 256)            1016064   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 2, 256)            1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 2, 1)              7937      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 2, 1)              4         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 2, 1)              0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                48        \n",
      "=================================================================\n",
      "Total params: 1,281,973\n",
      "Trainable params: 1,281,075\n",
      "Non-trainable params: 898\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python2.7/site-packages/ipykernel_launcher.py:34: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=Tensor(\"in...)`\n"
     ]
    }
   ],
   "source": [
    "#discriminator model\n",
    "inputs=Input((inputSize,1))\n",
    "# encoder\n",
    "#model.add(Reshape((16384,1,1),input_shape=input_shape2))\n",
    "d1=(Conv1D(64, 31, strides=4,padding='same'))\n",
    "d_hidden1=d1(inputs)\n",
    "d2=(BatchNormalization())\n",
    "d_hidden2=d2(d_hidden1)\n",
    "d3=(LeakyReLU(alpha=0.3))\n",
    "d_hidden3=d3(d_hidden2)\n",
    "d4=(Conv1D(128, 31, strides=4,padding='same'))\n",
    "d_hidden4=d4(d_hidden3)\n",
    "d5=(BatchNormalization())\n",
    "d_hidden5=d5(d_hidden4)\n",
    "d6=(LeakyReLU(alpha=0.3))\n",
    "d_hidden6=d6(d_hidden5)\n",
    "d7=(Conv1D(256, 31, strides=4,padding='same'))\n",
    "d_hidden7=d7(d_hidden6)\n",
    "d8=(BatchNormalization())\n",
    "d_hidden8=d8(d_hidden7)\n",
    "d9=(LeakyReLU(alpha=0.3))\n",
    "d_hidden9=d9(d_hidden8)\n",
    "d10=(Conv1D(1, 31, strides=1,padding='same'))\n",
    "d_hidden10=d10(d_hidden9)\n",
    "d11=(BatchNormalization())\n",
    "d_hidden11=d11(d_hidden10)\n",
    "d12=(LeakyReLU(alpha=0.3))\n",
    "d_hidden12=d12(d_hidden11)\n",
    "d13=(Flatten())\n",
    "d_hidden13=d13(d_hidden12)\n",
    "d14=Dense(16,activation='sigmoid')   \n",
    "d_output =d14(d_hidden13)\n",
    "\n",
    "D= Model(input = inputs,output=d_output)\n",
    "D.compile(loss=D_loss, optimizer=optim)\n",
    "#D.compile(loss='mse', optimizer=optim)\n",
    "D.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_4 (InputLayer)             (None, 128, 1, 1)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_5 (InputLayer)             (None, 128, 1)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "model_1 (Model)                  (None, 128, 1)        142776289   input_4[0][0]                    \n",
      "                                                                   input_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)                (None, 32, 64)        2048        model_1[1][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 32, 64)        256         conv1d_1[1][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)        (None, 32, 64)        0           batch_normalization_1[1][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)                (None, 8, 128)        254080      leaky_re_lu_1[1][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 8, 128)        512         conv1d_2[1][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)        (None, 8, 128)        0           batch_normalization_2[1][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)                (None, 2, 256)        1016064     leaky_re_lu_2[1][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 2, 256)        1024        conv1d_3[1][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)        (None, 2, 256)        0           batch_normalization_3[1][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)                (None, 2, 1)          7937        leaky_re_lu_3[1][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNorm (None, 2, 1)          4           conv1d_4[1][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)        (None, 2, 1)          0           batch_normalization_4[1][0]      \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 2)             0           leaky_re_lu_4[1][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 16)            48          flatten_1[1][0]                  \n",
      "====================================================================================================\n",
      "Total params: 144,058,262\n",
      "Trainable params: 142,776,289\n",
      "Non-trainable params: 1,281,973\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python2.7/site-packages/ipykernel_launcher.py:21: UserWarning: Update your `Model` call to the Keras 2 API: `Model([<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "make_trainable(D,False)\n",
    "inputs=Input(shape = (inputSize,1,1))\n",
    "inputs1=Input(shape = (inputSize,1))\n",
    "input=([inputs,inputs1])\n",
    "g_output = G(input)\n",
    "gan_hidden = d1(g_output)\n",
    "gan_hidden = d2(gan_hidden)\n",
    "gan_hidden = d3(gan_hidden)\n",
    "gan_hidden = d4(gan_hidden)\n",
    "gan_hidden = d5(gan_hidden)\n",
    "gan_hidden = d6(gan_hidden)\n",
    "gan_hidden = d7(gan_hidden)\n",
    "gan_hidden = d8(gan_hidden)\n",
    "gan_hidden = d9(gan_hidden)\n",
    "gan_hidden = d10(gan_hidden)\n",
    "gan_hidden = d11(gan_hidden)\n",
    "gan_hidden = d12(gan_hidden)\n",
    "gan_hidden = d13(gan_hidden)\n",
    "gan_output = d14(gan_hidden)\n",
    "\n",
    "GAN =Model([inputs,inputs1],output=gan_output)\n",
    "GAN.compile(loss=GAN_loss(g_output,inputs1), optimizer=optim) \n",
    "GAN.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32000, 128)\n",
      "(32000, 128)\n"
     ]
    }
   ],
   "source": [
    "# # --------------------load data--------------------\n",
    "import librosa\n",
    "i = 0\n",
    "import os\n",
    "cleanSignal = []\n",
    "noisySignal = []\n",
    "inputSizeSelect = 4096\n",
    "dataset_dir = \"musicDataset\"\n",
    "noise, sr = librosa.load(\"noise.wav\", duration=2.0)\n",
    "for filename in os.listdir(dataset_dir):\n",
    "     y, sr = librosa.load(dataset_dir+\"//\" + filename, duration=2.0)\n",
    "     yArr = np.array(y[0:inputSizeSelect])\n",
    "     ySplit = np.split(yArr, len(yArr)/128)\n",
    "#      print(noise)\n",
    "     y1 = y + noise\n",
    "#      print(len(y1))\n",
    "     y1Arr = np.array(y1[0:inputSizeSelect])\n",
    "     y1Split = np.split(y1Arr, len(y1Arr)/128)\n",
    "     cleanSignal = cleanSignal + ySplit\n",
    "     noisySignal = noisySignal + y1Split\n",
    "#      print(y.shape)\n",
    "#      noisySignal.append(y1[0:inputSize])\n",
    "cleanSignal = np.array(cleanSignal)\n",
    "noisySignal = np.array(noisySignal)\n",
    "\n",
    "print(cleanSignal.shape)\n",
    "print(noisySignal.shape)\n",
    "\n",
    "clean = cleanSignal\n",
    "noisy = noisySignal\n",
    "#noisy=clean+1.5*noise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --------------------Main Code--------------------\n",
    "batch_size=128\n",
    "n_epochs = 20\n",
    "n_minibatches = int(noisy.shape[0]/batch_size)\n",
    "\n",
    "#----------------------  load data ------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch:', 1)\n",
      "('minibatch:', 0)\n",
      "('minibatch:', 32)\n",
      "('minibatch:', 64)\n",
      "('minibatch:', 96)\n",
      "('minibatch:', 128)\n",
      "('minibatch:', 160)\n",
      "('minibatch:', 192)\n",
      "('minibatch:', 224)\n",
      "--------------------Discriminator trained!------------------\n",
      "0.0420586\n",
      "--------------------GAN trained!----------------------------\n",
      "1.07937\n",
      "('Epoch:', 2)\n",
      "('minibatch:', 0)\n",
      "('minibatch:', 32)\n",
      "('minibatch:', 64)\n",
      "('minibatch:', 96)\n",
      "('minibatch:', 128)\n",
      "('minibatch:', 160)\n",
      "('minibatch:', 192)\n",
      "('minibatch:', 224)\n",
      "--------------------Discriminator trained!------------------\n",
      "0.0657632\n",
      "--------------------GAN trained!----------------------------\n",
      "1.0181\n",
      "('Epoch:', 3)\n",
      "('minibatch:', 0)\n",
      "('minibatch:', 32)\n",
      "('minibatch:', 64)\n",
      "('minibatch:', 96)\n",
      "('minibatch:', 128)\n",
      "('minibatch:', 160)\n",
      "('minibatch:', 192)\n",
      "('minibatch:', 224)\n",
      "--------------------Discriminator trained!------------------\n",
      "0.0990078\n",
      "--------------------GAN trained!----------------------------\n",
      "0.919006\n",
      "('Epoch:', 4)\n",
      "('minibatch:', 0)\n",
      "('minibatch:', 32)\n",
      "('minibatch:', 64)\n",
      "('minibatch:', 96)\n",
      "('minibatch:', 128)\n",
      "('minibatch:', 160)\n",
      "('minibatch:', 192)\n",
      "('minibatch:', 224)\n",
      "--------------------Discriminator trained!------------------\n",
      "0.107627\n",
      "--------------------GAN trained!----------------------------\n",
      "0.840942\n",
      "('Epoch:', 5)\n",
      "('minibatch:', 0)\n",
      "('minibatch:', 32)\n",
      "('minibatch:', 64)\n",
      "('minibatch:', 96)\n",
      "('minibatch:', 128)\n",
      "('minibatch:', 160)\n",
      "('minibatch:', 192)\n",
      "('minibatch:', 224)\n",
      "--------------------Discriminator trained!------------------\n",
      "0.106233\n",
      "--------------------GAN trained!----------------------------\n",
      "0.755646\n",
      "('Epoch:', 6)\n",
      "('minibatch:', 0)\n",
      "('minibatch:', 32)\n",
      "('minibatch:', 64)\n",
      "('minibatch:', 96)\n",
      "('minibatch:', 128)\n",
      "('minibatch:', 160)\n",
      "('minibatch:', 192)\n",
      "('minibatch:', 224)\n",
      "--------------------Discriminator trained!------------------\n",
      "0.110201\n",
      "--------------------GAN trained!----------------------------\n",
      "0.714306\n",
      "('Epoch:', 7)\n",
      "('minibatch:', 0)\n",
      "('minibatch:', 32)\n",
      "('minibatch:', 64)\n",
      "('minibatch:', 96)\n",
      "('minibatch:', 128)\n",
      "('minibatch:', 160)\n",
      "('minibatch:', 192)\n",
      "('minibatch:', 224)\n",
      "--------------------Discriminator trained!------------------\n",
      "0.0957966\n",
      "--------------------GAN trained!----------------------------\n",
      "0.66091\n",
      "('Epoch:', 8)\n",
      "('minibatch:', 0)\n",
      "('minibatch:', 32)\n",
      "('minibatch:', 64)\n",
      "('minibatch:', 96)\n",
      "('minibatch:', 128)\n",
      "('minibatch:', 160)\n",
      "('minibatch:', 192)\n",
      "('minibatch:', 224)\n",
      "--------------------Discriminator trained!------------------\n",
      "0.109979\n",
      "--------------------GAN trained!----------------------------\n",
      "0.612704\n",
      "('Epoch:', 9)\n",
      "('minibatch:', 0)\n",
      "('minibatch:', 32)\n",
      "('minibatch:', 64)\n",
      "('minibatch:', 96)\n",
      "('minibatch:', 128)\n",
      "('minibatch:', 160)\n",
      "('minibatch:', 192)\n",
      "('minibatch:', 224)\n",
      "--------------------Discriminator trained!------------------\n",
      "0.113141\n",
      "--------------------GAN trained!----------------------------\n",
      "0.56407\n",
      "('Epoch:', 10)\n",
      "('minibatch:', 0)\n",
      "('minibatch:', 32)\n",
      "('minibatch:', 64)\n",
      "('minibatch:', 96)\n",
      "('minibatch:', 128)\n",
      "('minibatch:', 160)\n",
      "('minibatch:', 192)\n",
      "('minibatch:', 224)\n",
      "--------------------Discriminator trained!------------------\n",
      "0.101439\n",
      "--------------------GAN trained!----------------------------\n",
      "0.545039\n",
      "('Epoch:', 11)\n",
      "('minibatch:', 0)\n",
      "('minibatch:', 32)\n",
      "('minibatch:', 64)\n",
      "('minibatch:', 96)\n",
      "('minibatch:', 128)\n",
      "('minibatch:', 160)\n",
      "('minibatch:', 192)\n",
      "('minibatch:', 224)\n",
      "--------------------Discriminator trained!------------------\n",
      "0.106145\n",
      "--------------------GAN trained!----------------------------\n",
      "0.546651\n",
      "('Epoch:', 12)\n",
      "('minibatch:', 0)\n",
      "('minibatch:', 32)\n",
      "('minibatch:', 64)\n",
      "('minibatch:', 96)\n",
      "('minibatch:', 128)\n",
      "('minibatch:', 160)\n",
      "('minibatch:', 192)\n",
      "('minibatch:', 224)\n",
      "--------------------Discriminator trained!------------------\n",
      "0.136053\n",
      "--------------------GAN trained!----------------------------\n",
      "0.512507\n",
      "('Epoch:', 13)\n",
      "('minibatch:', 0)\n",
      "('minibatch:', 32)\n",
      "('minibatch:', 64)\n",
      "('minibatch:', 96)\n",
      "('minibatch:', 128)\n",
      "('minibatch:', 160)\n",
      "('minibatch:', 192)\n",
      "('minibatch:', 224)\n",
      "--------------------Discriminator trained!------------------\n",
      "0.129083\n",
      "--------------------GAN trained!----------------------------\n",
      "0.502451\n",
      "('Epoch:', 14)\n",
      "('minibatch:', 0)\n",
      "('minibatch:', 32)\n",
      "('minibatch:', 64)\n",
      "('minibatch:', 96)\n",
      "('minibatch:', 128)\n",
      "('minibatch:', 160)\n",
      "('minibatch:', 192)\n",
      "('minibatch:', 224)\n",
      "--------------------Discriminator trained!------------------\n",
      "0.121298\n",
      "--------------------GAN trained!----------------------------\n",
      "0.583175\n",
      "('Epoch:', 15)\n",
      "('minibatch:', 0)\n",
      "('minibatch:', 32)\n",
      "('minibatch:', 64)\n",
      "('minibatch:', 96)\n",
      "('minibatch:', 128)\n",
      "('minibatch:', 160)\n",
      "('minibatch:', 192)\n",
      "('minibatch:', 224)\n",
      "--------------------Discriminator trained!------------------\n",
      "0.136787\n",
      "--------------------GAN trained!----------------------------\n",
      "0.507582\n",
      "('Epoch:', 16)\n",
      "('minibatch:', 0)\n",
      "('minibatch:', 32)\n",
      "('minibatch:', 64)\n",
      "('minibatch:', 96)\n",
      "('minibatch:', 128)\n",
      "('minibatch:', 160)\n",
      "('minibatch:', 192)\n",
      "('minibatch:', 224)\n",
      "--------------------Discriminator trained!------------------\n",
      "0.0952572\n",
      "--------------------GAN trained!----------------------------\n",
      "0.499497\n",
      "('Epoch:', 17)\n",
      "('minibatch:', 0)\n",
      "('minibatch:', 32)\n",
      "('minibatch:', 64)\n",
      "('minibatch:', 96)\n",
      "('minibatch:', 128)\n",
      "('minibatch:', 160)\n",
      "('minibatch:', 192)\n",
      "('minibatch:', 224)\n",
      "--------------------Discriminator trained!------------------\n",
      "0.129591\n",
      "--------------------GAN trained!----------------------------\n",
      "0.535423\n",
      "('Epoch:', 18)\n",
      "('minibatch:', 0)\n",
      "('minibatch:', 32)\n",
      "('minibatch:', 64)\n",
      "('minibatch:', 96)\n",
      "('minibatch:', 128)\n",
      "('minibatch:', 160)\n",
      "('minibatch:', 192)\n",
      "('minibatch:', 224)\n",
      "--------------------Discriminator trained!------------------\n",
      "0.119116\n",
      "--------------------GAN trained!----------------------------\n",
      "0.52126\n",
      "('Epoch:', 19)\n",
      "('minibatch:', 0)\n",
      "('minibatch:', 32)\n",
      "('minibatch:', 64)\n",
      "('minibatch:', 96)\n",
      "('minibatch:', 128)\n",
      "('minibatch:', 160)\n",
      "('minibatch:', 192)\n",
      "('minibatch:', 224)\n",
      "--------------------Discriminator trained!------------------\n",
      "0.113216\n",
      "--------------------GAN trained!----------------------------\n",
      "0.574033\n",
      "('Epoch:', 20)\n",
      "('minibatch:', 0)\n",
      "('minibatch:', 32)\n",
      "('minibatch:', 64)\n",
      "('minibatch:', 96)\n",
      "('minibatch:', 128)\n",
      "('minibatch:', 160)\n",
      "('minibatch:', 192)\n",
      "('minibatch:', 224)\n",
      "--------------------Discriminator trained!------------------\n",
      "0.112395\n",
      "--------------------GAN trained!----------------------------\n",
      "0.580791\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_epochs):\n",
    "        print ('Epoch:', i+1)\n",
    "        for index in range(n_minibatches):\n",
    "            if(index%32 ==0):\n",
    "                print(\"minibatch:\" , index)\n",
    "            noisy_batch = noisy[index*batch_size:(index+1)*batch_size]\n",
    "            real_batch = clean[index*batch_size:(index+1)*batch_size]\n",
    "            \n",
    "            noisy_batch=np.reshape(noisy_batch,(batch_size,inputSize,1,1))\n",
    "            real_batch=np.reshape(real_batch,(batch_size,inputSize,1))\n",
    "            \n",
    "            combined_G_batch=([noisy_batch,real_batch])\n",
    "            \n",
    "            fake_batch = G.predict(combined_G_batch)\n",
    "            \n",
    "            fake_batch=np.reshape(fake_batch,(batch_size,inputSize))\n",
    "            real_batch=np.reshape(real_batch,(batch_size,inputSize))\n",
    "            \n",
    "            combined_X_batch = np.concatenate((real_batch, fake_batch))\n",
    "            one_label=np.ones([batch_size, 16])\n",
    "            zero_label=np.zeros([batch_size, 16])\n",
    "            combined_y_batch =np.vstack((one_label,zero_label))\n",
    "\n",
    "            make_trainable(D,True)\n",
    "            combined_X_batch=np.reshape(combined_X_batch,(2*batch_size,inputSize,1))\n",
    "            d_loss = D.train_on_batch(combined_X_batch, combined_y_batch)\n",
    "            \n",
    "            \n",
    "            make_trainable(D,False)\n",
    "            g_loss = GAN.train_on_batch(combined_G_batch,one_label)\n",
    "            \n",
    "#         print('--------------------enhanced speech Generated!--------------')\n",
    "        print('--------------------Discriminator trained!------------------')\n",
    "        print(d_loss)\n",
    "        print('--------------------GAN trained!----------------------------')\n",
    "        print(g_loss)\n",
    "        \n",
    "        if(i%10 ==0):\n",
    "            G.save_weights('cnn_generator_weights_' + str(n_epochs)+'_.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check in the values : by comparing the difference between fake and real vs noise and real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_batch = G.predict(combined_G_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.sum(abs(fake_batch[3] - real_batch[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.sum(abs(noise[0:128]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.7193813324\n"
     ]
    }
   ],
   "source": [
    "print(a/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.20816\n"
     ]
    }
   ],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest, srtest = librosa.load(dataset_dir + \"//blues.00000.au\", duration=8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "step = 128\n",
    "inp = []\n",
    "y2 = []\n",
    "b = noise[128:256]\n",
    "b = np.array(b)\n",
    "u = int(len(y)/128)\n",
    "while i<=u-1:\n",
    "    inp.append(ytest[i*step: i*step +step])\n",
    "    c = np.array(inp[i])\n",
    "    #c = np.array(inp[i]) + b\n",
    "    y2.append(c + b)\n",
    "    i+=1\n",
    "print(len(c))\n",
    "len(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344\n"
     ]
    }
   ],
   "source": [
    "print(np.array(y2).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = np.array(y2).shape[0]\n",
    "#fake_batch = G.predict(combined_G_batch)\n",
    "\n",
    "y2=np.reshape(np.array(y2),(size,inputSize,1,1))\n",
    "inp=np.reshape(np.array(inp),(size,inputSize,1))\n",
    "\n",
    "x=([y2,inp])\n",
    "\n",
    "fake_batch = G.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44032, 1, 1)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_test = np.concatenate(y2, axis = 0)\n",
    "\n",
    "# for y in y2:\n",
    "#      np.hstack((noise_test, y)\n",
    "noise_test.shape\n",
    "# librosa.output.write_wav('file_trim_2s.wav', y2, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_Test = np.concatenate(fake_batch, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_test = truth_Test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_n=np.reshape(np.array(noise_test),(size_test,1))\n",
    "test_t=np.reshape(np.array(truth_Test),(size_test,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.output.write_wav('DiffNoiseGanBefore.wav', test_n , sr)\n",
    "librosa.output.write_wav('DiffNoiseGanAfter.wav', test_t , sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
